{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import copy\n",
    "import gzip\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('.mplstyle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"LAMA/data/\"\n",
    "METRICS_DIR = os.path.join(BASE_DIR, \"metrics/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_metrics(results):\n",
    "    \"\"\"Average the metrics over samples\"\"\"\n",
    "    metrics = {'precision': {}, 'recall': {}}\n",
    "    for k in (1, 5, 10,  50, 100):\n",
    "        if type(list(results[0]['precision'].keys())[0]) == str:\n",
    "            k = str(k)\n",
    "        metrics['precision'][k] = np.mean([res['precision'][k] for res in results])\n",
    "        metrics['recall'][k] = np.mean([res['recall'][k] for res in results])\n",
    "    metrics['mrr'] = np.mean([res['rr'] for res in results])\n",
    "    metrics['samples'] = results\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_to_df(metrics):\n",
    "    data = []\n",
    "    for method in ('bm25plus', 'random'):\n",
    "        for eval_type in ('collapse', 'full'):\n",
    "            for metric in ('precision', 'recall', 'mrr'):\n",
    "                metric_result = metrics['evals'][method][eval_type][metric]\n",
    "                try:\n",
    "                    for k, score in metric_result.items():\n",
    "                        data.append((method, 'local', 'dot', eval_type, metric, str(k), score))\n",
    "                except:\n",
    "                    data.append((method, 'local', 'dot', eval_type, metric, \"1\", metric_result))\n",
    "                              \n",
    "    for method_type in ('local', 'global'):\n",
    "        for eval_type in ('collapse', 'full'):\n",
    "            for normalized in ('cosine', 'dot'):\n",
    "                if method_type == \"global\" and normalized == \"dot\":\n",
    "                    continue\n",
    "                for method, method_results in metrics['evals'][method_type][normalized][eval_type].items():\n",
    "                    for metric in ('precision', 'recall', 'mrr'):\n",
    "                        metric_result = method_results[metric]\n",
    "                        try:\n",
    "                            for k, score in metric_result.items():\n",
    "                                data.append((method, method_type, normalized, eval_type, metric, str(k), score))\n",
    "                        except:\n",
    "                            data.append((method, method_type, normalized, eval_type, metric, \"1\", metric_result))\n",
    "                            \n",
    "    df = pd.DataFrame(data, columns=['layers', 'norm_type', 'normalization', 'eval', 'metrics', 'k', 'score'])                   \n",
    "    \n",
    "    df['layer_type'] = 'A'\n",
    "    df.loc[df['layers'].str.contains('gradients'), 'layer_type'] = 'gradients'\n",
    "    df.loc[(df['layers'].str.contains('gradients')) & (df['layers'].str.contains('activations')), 'layer_type'] = 'gradients_and_activations'\n",
    "    df.loc[(df['layers'] == 'random') | (df['layers'] == 'bm25plus'), 'layer_type'] = 'baseline'\n",
    "    \n",
    "    df = df.replace({'gradients':'G', \n",
    "                     'activations': 'A',\n",
    "                     'block.': '', \n",
    "                     'encoder': 'E', \n",
    "                     'decoder': 'D', \n",
    "                     'shared': 'emb', \n",
    "                     'random': 'Target-Picker'}, regex=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_one_experiment(paths, \n",
    "                             suffix=\"\", \n",
    "                             folder=\"plots/\",\n",
    "                             show=True,\n",
    "                             return_df=False):\n",
    "  \n",
    "    dfs = []\n",
    "    for (i, path) in enumerate(paths):\n",
    "        with gzip.open(METRICS_DIR + path) as f:\n",
    "            reranker_metrics = pickle.load(f)\n",
    "            \n",
    "        df = metrics_to_df(reranker_metrics)\n",
    "        df = df[~df['layers'].str.contains('A.E.0,A.D.0,')]\n",
    "        df['seed'] = i\n",
    "        dfs.append(df)\n",
    "      \n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    # scores = df.groupby(['normalization', 'eval', 'layers', 'metrics', 'k']).agg({'score': ['mean', 'std']}, as_index=False)\n",
    "    \n",
    "    # return df, scores\n",
    "    \n",
    "    \n",
    "    if return_df:\n",
    "        return df\n",
    "    for method in (\"full\", \"collapse\"):\n",
    "      \n",
    "      plot_with_filter(df, \n",
    "                       filter=lambda x: x[(x['metrics'] == 'precision') & (x['k'] == '3')  & (x['eval'] == method)],\n",
    "                       title=f'precision@3 ({method} + {suffix})',\n",
    "                       folder=folder,\n",
    "                       show=show)\n",
    "      \n",
    "      plot_with_filter(df,\n",
    "                       filter=lambda x: x[(x['metrics'] == 'recall') & (x['k'] == '3') & (x['eval'] == method)],\n",
    "                       title=f'recall@3 ({method} + {suffix})',\n",
    "                       folder=folder,\n",
    "                       show=show)\n",
    "      \n",
    "      plot_with_filter(df,\n",
    "                       filter=lambda x: x[(x['metrics'] == 'mrr') & (x['k'] == '1') & (x['eval'] == method)],\n",
    "                       title=f'mrr ({method} + {suffix})',\n",
    "                       folder=folder,\n",
    "                       show=show)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf  = visualize_one_experiment(paths=[f'reranker/unfiltered/seed_{i}/learned/no_eos_accum/results.entity.pickle'\n",
    "                                     for i in range(1)\n",
    "                                     ], \n",
    "                              suffix=\"\", \n",
    "                              show=False, \n",
    "                              return_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf[(sdf['norm_type'] == 'local') & (sdf['normalization'] == 'cosine')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf[(sdf['norm_type'] == 'global') & (sdf['normalization'] == 'cosine')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf['layer_type'] = sdf['layer_type'].replace({'G': 'TracIn', \n",
    "                           'A': 'Embed', \n",
    "                           'G_and_A': 'TracIn+Embed',\n",
    "                           'baseline': 'Baselines'\n",
    "                          })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf['layers'] = sdf['layers'].replace({f'G.E.{i},G.D.{i}': f'G.E.{i+1},G.D.{i+1}' for i in range(12)})\\\n",
    "             .replace({f'G.E.{i}': f'G.E.{i+1}' for i in range(12)})\\\n",
    "             .replace({f'G.emb,G.E.{i},G.D.{i}': f'G.emb,G.E.{i+1},G.D.{i+1}' for i in range(12)})\\\n",
    "             .replace({f'G.emb,G.E.{i}': f'G.emb,G.E.{i+1}' for i in range(12)})\\\n",
    "             .str.replace('G.emb', 'G.0').str.replace('bm25plus','BM25+').str.replace('Target-Picker', 'Random-Target')\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_filter(df, filter=lambda x: x, title=\"Title\", folder=\"plots/\", show=True):\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    ax = sns.barplot(data=filter(df), \n",
    "                x='layers', \n",
    "                y='score', \n",
    "                hue='layer_type', \n",
    "                estimator=np.mean, \n",
    "                ci='sd')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Layer Selection\")\n",
    "    plt.ylabel(\"Recall@10\")\n",
    "    plt.legend(title='Method Type')\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(folder+title+\"_plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_with_filter(sdf,\n",
    "                filter=lambda x: x[(x['metrics'] == 'mrr') & (x['k'] == '1') & (x['eval'] == 'collapse') & (x['normalization'] == 'cosine') & (x['norm_type'] == \"local\")],\n",
    "                title='',\n",
    "                show=False,\n",
    "                folder=\"./paper_figures/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics['evals']['global']['cosine']['full']['gradients.']['precision']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics['evals']['local']['cosine']['full']['gradients.']['precision']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = {}\n",
    "for norm_type in (\"ln\", ):\n",
    "    for eos in (\"eos\", ):\n",
    "        for subset in (\"learned\", ):\n",
    "            for accum in (\"no_accum\", \"accum\"):\n",
    "                try:\n",
    "                    suffix=f\"{norm_type}+{eos}+{subset}+{accum}\"\n",
    "                    df[suffix]=visualize_one_experiment(paths=[f'/reranker/exp_layers_{i}/{norm_type}_sl_{eos}__{subset}_{accum}.json'\n",
    "                                                               for i in range(2)], \n",
    "                                                        suffix=suffix, \n",
    "                                                        show=False, \n",
    "                                                        return_df=True)\n",
    "                except FileNotFoundError:\n",
    "                    print(f'notfound: /reranker/exp_layers/{norm_type}_sl_{eos}__{subset}_{accum}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (k, v) in df.items():\n",
    "    norm_type, eos, subset, accum = k.split('+')\n",
    "    v['norm_type'] = norm_type\n",
    "    v['eos'] = eos\n",
    "    v['subset'] = subset\n",
    "    v['accum'] = accum\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmerged = pd.concat(df.values(), ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmerged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = dfmerged.groupby(['normalization', 'eval', 'layers', 'metrics', 'k', 'layer_type', 'norm_type', 'eos', 'subset', 'accum']).agg({'score': ['mean', 'std']}, as_index=False)\n",
    "scores = scores.reset_index()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max(df: pd.DataFrame, prefix, no_prefix=None, subset='learned', metric='mrr', eval_type='collapse', k='1'):\n",
    "    df2 = df[(df['layers'].str.startswith(prefix)) & (df['subset'] == subset) & (df['metrics'] == metric) & (df['k'] == str(k)) & (df['eval'] == eval_type)]\n",
    "    if no_prefix:\n",
    "        df2 = df2[~df2['layers'].str.contains(no_prefix)]\n",
    "    nlargest = df2.nlargest(20, ('score', 'mean'))\n",
    "    largest = nlargest.iloc[0]    \n",
    "    dflargest = df[(df['layers'] == largest['layers'][0]) & (df['norm_type'] == largest['norm_type'][0]) & (df['eos'] == largest['eos'][0]) &  (df['accum'] == largest['accum'][0]) &(df['subset'] == subset) & (df['eval'] == eval_type)]\n",
    "    return nlargest, dflargest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest, dflargest = get_max(scores, 'A', no_prefix='G')\n",
    "display(largest)\n",
    "display(dflargest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest, dflargest = get_max(scores, 'A')\n",
    "display(largest)\n",
    "display(dflargest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest, dflargest = get_max(scores, 'G')\n",
    "display(largest)\n",
    "display(dflargest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest, dflargest = get_max(scores, 'bm25')\n",
    "display(largest)\n",
    "display(dflargest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest, dflargest  = get_max(scores, 'Target-Picker')\n",
    "display(largest)\n",
    "display(dflargest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest, dflargest = get_max(scores, 'A', no_prefix='G', subset='random')\n",
    "display(largest)\n",
    "display(dflargest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest, dflargest = get_max(scores, 'A', subset='random')\n",
    "display(largest)\n",
    "display(dflargest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest, dflargest = get_max(scores, 'G', subset='random')\n",
    "display(largest)\n",
    "display(dflargest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest, dflargest = get_max(scores, 'bm25', subset='random')\n",
    "display(largest)\n",
    "display(dflargest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest, dflargest = get_max(scores, 'Target-Picker', subset='random')\n",
    "display(largest)\n",
    "display(dflargest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval on correct samples (+eos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for norm_type in (\"ln\", \"gn\"):\n",
    "    for eos in (\"no_eos\", ):\n",
    "        for subset in (\"learned\", ):\n",
    "            for accum in (\"accum\", ):\n",
    "                try:\n",
    "                    # visualize_one_experiment(path=f'/reranker/{norm_type}_sl_{eos}__{subset}_{accum}.json', suffix=f\"{norm_type} + {eos} + {subset} + {accum}\", show=False, folder=\"sentence_level_all_plots/\")\n",
    "                    visualize_one_experiment(path=f'/reranker/{norm_type}_sl_{eos}__{subset}_{accum}.json', \n",
    "                                             suffix=f\"{norm_type} + {eos} + {subset} + {accum}\", \n",
    "                                             show=True)\n",
    "                except FileNotFoundError:\n",
    "                    print(f'notfound: /reranker/{norm_type}_sl_{eos}__{subset}_{accum}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_nn_abstracts(res, config=\"\"):\n",
    "    \"\"\"Get list of abstracts and their scores (dotproduct score)\"\"\"\n",
    "    print(f\"config: {config}\")\n",
    "    example = res['example']\n",
    "    print(example['inputs_pretokenized'],\" -> \", example['targets_pretokenized'])\n",
    "    fact = (example['predicate_id'].strip(), example['obj_uri'].strip(), example['sub_uri'].strip())\n",
    "    print(fact)\n",
    "    abstracts = np.array(res['nn_abstracts'])\n",
    "    for i, abstract in enumerate(abstracts):\n",
    "        try:\n",
    "            abstract['score'] = res['nn_scores'][i]\n",
    "        except KeyError:\n",
    "            abstract['score'] = res['nn']['scores'][i]\n",
    "    df = pd.DataFrame(pd.json_normalize(abstracts)).round(3)\n",
    "    # df['inputs_pretokenized'] = df['inputs_pretokenized'].str.slice(0,512)\n",
    "    df['label'] = df['facts'].str.contains(\",\".join(fact))\n",
    "    df = df.drop(['page_uri', 'masked_uri', 'masked_type', 'facts', 'sentence_uris'], axis=1)\n",
    "    return df\n",
    "\n",
    "def result_getter(path):\n",
    "    with open(METRICS_DIR + path) as f:\n",
    "            reranker_metrics = json.load(f)\n",
    "      \n",
    "    def getter(i=3,\n",
    "               sim=\"cosine\",\n",
    "               method=\"full\",\n",
    "               layers='activations.encoder.block.11,activations.decoder.block.11'):\n",
    "        config = {\"sim\": sim, \"method\": method, \"layers\": layers}\n",
    "        return get_nn_abstracts(reranker_metrics[sim][method][layers]['samples'][i], config=config)\n",
    "          \n",
    "    getter.metrics = reranker_metrics\n",
    "    return getter\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 25)\n",
    "pd.set_option('display.max_colwidth', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getter = result_getter('/reranker/exp_layers_0/ln_sl_no_eos__learned_no_accum.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def side_by_side(df1, df2, df3):\n",
    "    df = pd.DataFrame(columns=['TracIn','Embed','BM25', ])\n",
    "    for dfi in (df1, df2, df3):\n",
    "        dfi['sample'] = \"\\textbf{Q:} \" + dfi['inputs_pretokenized'] + \"\\n\\textbf{A:} \" + dfi['targets_pretokenized'].replace('<extra_id_0> ','') + \"\\n\\textbf{\" + dfi['label'].astype(str) + \"}\"\n",
    "        dfi['sample'] = dfi['sample'].str.replace(\"<extra_id_0>\", \"[MASK]\")\n",
    "    df['TracIn'] = df1['sample']\n",
    "    df['Embed'] = df2['sample']\n",
    "    df['BM25'] = df3['sample']\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(40, 45):\n",
    "    dfside = side_by_side(getter(layers='gradients.encoder.block.1,gradients.decoder.block.1', i=i),\n",
    "                 getter(layers='activations.encoder.block.0,activations.decoder.block.0', i=i),\n",
    "                 getter(layers='bm25plus', i=i)).head(5)\n",
    "    # Assuming the variable df contains the relevant DataFrame\n",
    "    display(dfside.style.set_properties(**{\n",
    "        'text-align': 'left',\n",
    "        'white-space': 'pre-wrap',\n",
    "    }))\n",
    "    print(dfside.to_latex(index=False, escape=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=21\n",
    "display(getter(layers='gradients.shared', i=i).head(5))\n",
    "display(getter(layers='activations.encoder.block.0,activations.decoder.block.0', i=i).head(5))\n",
    "display(getter(layers='bm25plus', i=i).head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getter2 = result_getter('/reranker/exp_layers_0/ln_sl_no_eos__learned_accum.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(getter2(layers='gradients.shared'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getter = result_getter('/reranker/exp_layers/gn_sl_eos__learned_accum.json')\n",
    "display(getter(layers='activations.encoder.block.11,activations.decoder.block.11'))\n",
    "display(getter(layers='activations.encoder.block.0,activations.decoder.block.0'))\n",
    "display(getter(layers='gradients.encoder.block.11,gradients.decoder.block.11'))\n",
    "display(getter(layers='gradients.shared'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getter = result_getter('/reranker/exp_layers/gn_sl_eos__corrects_accum.json')\n",
    "# display(getter(layers='activations.encoder.block.11,activations.decoder.block.11'))\n",
    "# display(getter(layers='activations.encoder.block.0,activations.decoder.block.0'))\n",
    "# display(getter(layers='gradients.encoder.block.11,gradients.decoder.block.11'))\n",
    "# display(getter(layers='gradients.shared'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(getter.metrics['cosine']['full']['bm25plus']['samples'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python-3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
