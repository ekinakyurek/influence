{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import gzip\n",
    "import pickle\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "plt.style.use(\".mplstyle\")\n",
    "pd.set_option(\"display.max_rows\", 25)\n",
    "pd.set_option(\"display.max_colwidth\", 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"LAMA/data/\"\n",
    "METRICS_DIR = os.path.join(BASE_DIR, \"metrics/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_to_df(results, metrics=[\"precision\", \"recall\", \"mrr\"]):\n",
    "    data = []\n",
    "\n",
    "    for method in (\"bm25plus\", \"random\"):\n",
    "        for eval_type in (\"collapse\", \"full\"):\n",
    "            if eval_type != \"collapse\":\n",
    "                current_metrics = metrics + [\"mrr_compare_fn_subject\", \"mrr_compare_fn_object\", \"mrr_compare_fn_relation\"]\n",
    "            else:\n",
    "                current_metrics = metrics\n",
    "            for metric in current_metrics:\n",
    "                metric_result = results[\"evals\"][method][eval_type][metric]\n",
    "                for normalized in (\"cosine\", \"dot\"):\n",
    "                    for method_type in (\"local\", \"global\"):\n",
    "                        try:\n",
    "                            for k, score in metric_result.items():\n",
    "                                data.append(\n",
    "                                    (\n",
    "                                        method,\n",
    "                                        method_type,\n",
    "                                        normalized,\n",
    "                                        eval_type,\n",
    "                                        metric,\n",
    "                                        str(k),\n",
    "                                        score,\n",
    "                                    )\n",
    "                                )\n",
    "                        except:\n",
    "                            data.append(\n",
    "                                (\n",
    "                                    method,\n",
    "                                    method_type,\n",
    "                                    normalized,\n",
    "                                    eval_type,\n",
    "                                    metric,\n",
    "                                    \"1\",\n",
    "                                    metric_result,\n",
    "                                )\n",
    "                            )\n",
    "\n",
    "    for method_type in (\"local\", \"global\"):\n",
    "        for eval_type in (\"collapse\", \"full\"):\n",
    "            for normalized in (\"cosine\", \"dot\"):\n",
    "                if method_type == \"global\" and normalized == \"dot\":\n",
    "                    continue\n",
    "                for method, method_results in results[\"evals\"][method_type][\n",
    "                    normalized\n",
    "                ][eval_type].items():\n",
    "                    if eval_type != \"collapse\":\n",
    "                        current_metrics = metrics + [\"mrr_compare_fn_subject\", \"mrr_compare_fn_object\", \"mrr_compare_fn_relation\"]\n",
    "                    else:\n",
    "                        current_metrics = metrics\n",
    "                    for metric in current_metrics:\n",
    "                        metric_result = method_results[metric]\n",
    "                        try:\n",
    "                            for k, score in metric_result.items():\n",
    "                                data.append(\n",
    "                                    (\n",
    "                                        method,\n",
    "                                        method_type,\n",
    "                                        normalized,\n",
    "                                        eval_type,\n",
    "                                        metric,\n",
    "                                        str(k),\n",
    "                                        score,\n",
    "                                    )\n",
    "                                )\n",
    "                        except:\n",
    "                            data.append(\n",
    "                                (\n",
    "                                    method,\n",
    "                                    method_type,\n",
    "                                    normalized,\n",
    "                                    eval_type,\n",
    "                                    metric,\n",
    "                                    \"1\",\n",
    "                                    metric_result,\n",
    "                                )\n",
    "                            )\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        data,\n",
    "        columns=[\n",
    "            \"layers\",\n",
    "            \"norm_type\",\n",
    "            \"normalization\",\n",
    "            \"eval\",\n",
    "            \"metrics\",\n",
    "            \"k\",\n",
    "            \"score\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    df[\"layer_type\"] = \"Embed\"\n",
    "    df.loc[df[\"layers\"].str.contains(\"gradients\"), \"layer_type\"] = \"TracIn\"\n",
    "    df.loc[\n",
    "        (df[\"layers\"].str.contains(\"gradients\"))\n",
    "        & (df[\"layers\"].str.contains(\"activations\")),\n",
    "        \"layer_type\",\n",
    "    ] = \"TracIn+Embed\"\n",
    "    df.loc[\n",
    "        (df[\"layers\"] == \"random\") | (df[\"layers\"] == \"bm25plus\"), \"layer_type\"\n",
    "    ] = \"baselines\"\n",
    "\n",
    "    df = df.replace(\n",
    "        {\n",
    "            \"gradients\": \"G\",\n",
    "            \"activations\": \"A\",\n",
    "            \"block.\": \"\",\n",
    "            \"encoder\": \"E\",\n",
    "            \"decoder\": \"D\",\n",
    "            \"shared\": \"emb\",\n",
    "            \"random\": \"Target-Picker\",\n",
    "        },\n",
    "        regex=True,\n",
    "    )\n",
    "\n",
    "    df[\"layers\"] = (\n",
    "        df[\"layers\"]\n",
    "        .replace(\n",
    "            {f\"G.E.{i},G.D.{i}\": f\"G.E.{i+1},G.D.{i+1}\" for i in range(12)},\n",
    "            regex=False,\n",
    "        )\n",
    "        .replace({f\"G.E.{i}\": f\"G.E.{i+1}\" for i in range(12)}, regex=False)\n",
    "        .replace(\n",
    "            {\n",
    "                f\"G.emb,G.E.{i},G.D.{i}\": f\"G.emb,G.E.{i+1},G.D.{i+1}\"\n",
    "                for i in range(12)\n",
    "            },\n",
    "            regex=False,\n",
    "        )\n",
    "        .replace(\n",
    "            {f\"G.emb,G.E.{i}\": f\"G.emb,G.E.{i+1}\" for i in range(12)},\n",
    "            regex=False,\n",
    "        )\n",
    "        .str.replace(\"G.emb\", \"G.0\", regex=False)\n",
    "        .str.replace(\"bm25plus\", \"BM25+\", regex=False)\n",
    "        .str.replace(\"Target-Picker\", \"Random-Target\", regex=False)\n",
    "    )\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_filter(df, \n",
    "                     filter=lambda x: x,\n",
    "                     title=\"Title\",\n",
    "                     folder=\"plots/\",\n",
    "                     ylabel='',\n",
    "                     save=False):\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    \n",
    "    ax = sns.barplot(data=filter(df), \n",
    "                     x='layers', \n",
    "                     y='score', \n",
    "                     hue='layer_type', \n",
    "                     estimator=np.mean, \n",
    "                     ci='sd')\n",
    "    \n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Layer Selection\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend(title='Method Type')\n",
    "    \n",
    "    if not save:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(folder+title+\"_plot.png\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_one_experiment(\n",
    "    paths,\n",
    "    suffix=\"\",\n",
    "    folder=\"plots/\",\n",
    "    save=False,\n",
    "    visualize=False,\n",
    "    k=\"3\",\n",
    "    norm_type=\"local\",\n",
    "    normalization=\"cosine\",\n",
    "):\n",
    "    dfs = []\n",
    "    for (i, path) in enumerate(paths):\n",
    "\n",
    "        with gzip.open(METRICS_DIR + path) as f:\n",
    "            reranker_metrics = pickle.load(f)\n",
    "\n",
    "        df = metrics_to_df(reranker_metrics)\n",
    "        df[\"seed\"] = i\n",
    "        dfs.append(df)\n",
    "\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    if visualize:\n",
    "\n",
    "        # scores = df.groupby(['normalization', 'eval', 'layers', 'metrics', 'k']).agg({'score': ['mean', 'std']}, as_index=False)\n",
    "        # Layers that we don't want for visualizations\n",
    "        ddf = df[~df[\"layers\"].str.contains(\"A.E.0,A.D.0,\")]\n",
    "\n",
    "        for method in (\"full\", \"collapse\"):\n",
    "            plot_with_filter(\n",
    "                ddf,\n",
    "                filter=lambda x: x[\n",
    "                    (x[\"metrics\"] == \"precision\")\n",
    "                    & (x[\"k\"] == '3')\n",
    "                    & (x[\"eval\"] == method)\n",
    "                    & (x[\"norm_type\"] == norm_type)\n",
    "                    & (x[\"normalization\"] == normalization)\n",
    "                ],\n",
    "                title=f\"precision@3 ({method} + {suffix})\",\n",
    "                folder=folder,\n",
    "                ylabel=\"precision@3\",\n",
    "                save=save,\n",
    "            )\n",
    "\n",
    "            plot_with_filter(\n",
    "                ddf,\n",
    "                filter=lambda x: x[\n",
    "                    (x[\"metrics\"] == \"recall\")\n",
    "                    & (x[\"k\"] == '10')\n",
    "                    & (x[\"eval\"] == method)\n",
    "                    & (x[\"norm_type\"] == norm_type)\n",
    "                    & (x[\"normalization\"] == normalization)\n",
    "                ],\n",
    "                title=f\"recall@10 ({method} + {suffix})\",\n",
    "                folder=folder,\n",
    "                ylabel=\"recall@10\",\n",
    "                save=save,\n",
    "            )\n",
    "\n",
    "            plot_with_filter(\n",
    "                ddf,\n",
    "                filter=lambda x: x[\n",
    "                    (x[\"metrics\"] == \"mrr\")\n",
    "                    & (x[\"k\"] == \"1\")\n",
    "                    & (x[\"eval\"] == method)\n",
    "                    & (x[\"norm_type\"] == norm_type)\n",
    "                    & (x[\"normalization\"] == normalization)\n",
    "                ],\n",
    "                title=f\"mrr ({method} + {suffix})\",\n",
    "                folder=folder,\n",
    "                ylabel=\"mrr\",\n",
    "                save=save,\n",
    "            )\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_abstracts_df(res, fact):\n",
    "    abstracts = np.array(res[\"nn_abstracts\"])\n",
    "    for i, abstract in enumerate(abstracts):\n",
    "        try:\n",
    "            abstract[\"score\"] = res[\"nn_scores\"][i]\n",
    "        except KeyError:\n",
    "            abstract[\"score\"] = res[\"nn\"][\"scores\"][i]\n",
    "\n",
    "    df = pd.DataFrame(pd.json_normalize(abstracts)).round(3)\n",
    "    df[\"label\"] = df[\"facts\"].str.contains(\",\".join(fact))\n",
    "\n",
    "    df = df.drop(\n",
    "        [\"page_uri\", \"masked_uri\", \"masked_type\", \"facts\", \"example_uris\"],\n",
    "        axis=1,\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_nn_abstracts(res, baseline_res, config=\"\"):\n",
    "    \"\"\"Get list of abstracts and their scores (dotproduct score)\"\"\"\n",
    "    print(f\"Config: {config}\")\n",
    "    example = baseline_res[\"example\"]\n",
    "    print(\n",
    "        f\"Example: {example['inputs_pretokenized']} =>\"\n",
    "        f\" {example['targets_pretokenized']}\"\n",
    "    )\n",
    "    fact = (\n",
    "        example[\"predicate_id\"].strip(),\n",
    "        example[\"obj_uri\"].strip(),\n",
    "        example[\"sub_uri\"].strip(),\n",
    "    )\n",
    "    print(f\"Fact: {fact}\")\n",
    "    print(f\"Model Precision\", res[\"precision\"])\n",
    "    df_model = get_abstracts_df(res, fact)\n",
    "    print(f\"Baseline Precision\", baseline_res[\"precision\"])\n",
    "    df_baseline = get_abstracts_df(baseline_res, fact)\n",
    "    return df_model, df_baseline\n",
    "\n",
    "\n",
    "def result_getter(path):\n",
    "    with gzip.open(METRICS_DIR + path) as f:\n",
    "        reranker_metrics = pickle.load(f)\n",
    "\n",
    "    def getter(\n",
    "        i=3,\n",
    "        sim=\"cosine\",\n",
    "        method=\"collapse\",\n",
    "        normalization=\"local\",\n",
    "        layers=\"activations.encoder.block.0,gradients.shared\",\n",
    "    ):\n",
    "\n",
    "        config = {\n",
    "            \"sim\": sim,\n",
    "            \"method\": method,\n",
    "            \"layers\": layers,\n",
    "            \"normalization\": normalization,\n",
    "        }\n",
    "\n",
    "        return get_nn_abstracts(\n",
    "            reranker_metrics[\"evals\"][normalization][sim][method][layers][\n",
    "                \"samples\"\n",
    "            ][i],\n",
    "            reranker_metrics[\"samples\"][i],\n",
    "            config=config,\n",
    "        )\n",
    "\n",
    "    getter.metrics = reranker_metrics\n",
    "    return getter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getter = result_getter('reranker/sweep_v2/seed_0/learned/no_eos_accum/results_detailed.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 10\n",
    "tracin_res, baseline_res = getter(i=idx, layers='gradients.shared')\n",
    "embed_res,_= getter(i=idx, layers='activations.encoder.block.0,activations.decoder.block.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracin_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(model_res):\n",
    "    model_res['text'] = '\\\\textbf{Q:} ' + model_res['inputs_pretokenized'].str.replace('<extra_id_0>', '[MASK]') +  ' \\\\newline\\\\textbf{A:} ' + model_res['targets_pretokenized'].str.replace('<extra_id_0>', '') + ' \\\\textbf{' +  model_res['label'].astype(str) + '}'\n",
    "    return model_res.head(3)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\\\\\\\ \\n  \\\\midrule \\n\".join(process(tracin_res)['text'] + '&' + process(embed_res)['text'] +  '&' + process(baseline_res)['text'] ) + ' \\\\ \\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"Model Table\")\n",
    "display(model_res)\n",
    "print(\"Baseline Table\")\n",
    "display(baseline_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_res, baseline_res = getter(i=3, layers='activations.encoder.block.0,activations.decoder.block.0')\n",
    "print(\"Model Table\")\n",
    "display(model_res)\n",
    "print(\"Baseline Table\")\n",
    "display(baseline_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = visualize_one_experiment(\n",
    "    paths=[\n",
    "        f\"reranker/sweep_v2/seed_{i}/learned/no_eos_accum/results_detailed.pickle\"\n",
    "        for i in range(3)\n",
    "    ],\n",
    "    suffix=\"no_eos_accum\",\n",
    "    save=True,\n",
    "    visualize=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "for eos in (\"no_eos\", ):\n",
    "    for subset in (\"learned\",):\n",
    "        for accum in (\"no_accum\", \"accum\"):\n",
    "            try:\n",
    "                suffix = f\"{eos}+{subset}+{accum}\"\n",
    "                df = visualize_one_experiment(\n",
    "                    paths=[\n",
    "                        f\"reranker/sweep_v2/seed_{i}/{subset}/{eos}_{accum}/results_detailed.pickle\"\n",
    "                        for i in range(3)\n",
    "                    ],\n",
    "                    suffix=suffix,\n",
    "                    save=False,\n",
    "                    visualize=False,\n",
    "                )\n",
    "                df[\"eos\"] = eos\n",
    "                df[\"subset\"] = subset\n",
    "                df[\"accum\"] = accum\n",
    "                dfs[suffix] = df\n",
    "            except FileNotFoundError:\n",
    "                print(\n",
    "                    \"Couldn't find: \"\n",
    "                    f\"reranker/sweep_v2/seed_0/{subset}/{eos}_{accum}/results_detailed.pickle\"\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmerged = pd.concat(list(dfs.values()), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = dfmerged.groupby([column for column in dfmerged.columns if column != 'seed' and column != 'score'], axis=0).agg({'score': [np.mean, np.std]}, as_index=False)\n",
    "scores = scores.reset_index()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max(df: pd.DataFrame, \n",
    "            prefix, \n",
    "            no_prefix=None, \n",
    "            subset='learned', \n",
    "            metric='mrr', \n",
    "            eval_type='collapse', \n",
    "            k='1'):\n",
    "    \n",
    "    df2 = df[(df['layers'].str.startswith(prefix)) & \n",
    "             (df['subset'] == subset) &\n",
    "             (df['metrics'] == metric) &\n",
    "             (df['k'] == str(k)) &\n",
    "             (df['eval'] == eval_type)]\n",
    "    \n",
    "    if no_prefix:\n",
    "        df2 = df2[~df2['layers'].str.contains(no_prefix)]\n",
    "        \n",
    "    nlargest = df2.nlargest(10, ('score', 'mean'))\n",
    "    \n",
    "    largest = nlargest.iloc[0]    \n",
    "    dflargest = df[(df['layers'] == largest['layers'][0]) &\n",
    "                   (df['norm_type'] == largest['norm_type'][0]) &\n",
    "                   (df['eos'] == largest['eos'][0]) &\n",
    "                   (df['accum'] == largest['accum'][0]) &\n",
    "                   (df['subset'] == subset)]# &\n",
    "#                   (df['eval'] == eval_type)]\n",
    "\n",
    "    dflargest = dflargest.set_index('metrics').loc[['mrr', 'mrr_compare_fn_relation', 'mrr_compare_fn_subject', 'mrr_compare_fn_object', 'recall']]\n",
    "    dflargest['score_text'] = (100 * dflargest['score']['mean']).apply(lambda x: f\"{x:.2f}\") + '\\stderr{' + (100 * dflargest['score']['std']).apply(lambda x: f\"{x:.2f}\") + '}'\n",
    "    dflargest = dflargest.transpose()\n",
    "    dflargest = dflargest.loc[['score_text', 'k', 'eval']]\n",
    "    return nlargest, dflargest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest, dflargest = get_max(scores, 'A', no_prefix='G')\n",
    "display(largest)\n",
    "display(dflargest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest, dflargest = get_max(scores, 'A')\n",
    "display(largest)\n",
    "display(dflargest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest, dflargest = get_max(scores, 'G')\n",
    "display(largest)\n",
    "display(dflargest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest, dflargest = get_max(scores, 'BM25+')\n",
    "display(largest)\n",
    "display(dflargest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest, dflargest  = get_max(scores, 'Random-Target')\n",
    "display(largest)\n",
    "display(dflargest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "for eos in (\"no_eos\", \"eos\"):\n",
    "    for subset in (\"learned\",):\n",
    "        for accum in (\"no_accum\", \"accum\"):\n",
    "            try:\n",
    "                suffix = f\"{eos}+{subset}+{accum}\"\n",
    "                df = visualize_one_experiment(\n",
    "                    paths=[\n",
    "                        f\"reranker/sweep_v2/seed_{i}/{subset}/{eos}_{accum}/results_detailed.pickle\"\n",
    "                        for i in range(3)\n",
    "                    ],\n",
    "                    suffix=suffix,\n",
    "                    save=False,\n",
    "                    visualize=False,\n",
    "                )\n",
    "                df[\"eos\"] = eos\n",
    "                df[\"subset\"] = subset\n",
    "                df[\"accum\"] = accum\n",
    "                dfs[suffix] = df\n",
    "            except FileNotFoundError:\n",
    "                print(\n",
    "                    \"Couldn't find: \"\n",
    "                    f\"reranker/sweep_v2/seed_0/{subset}/{eos}_{accum}/results_detailed.pickle\"\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmerged = pd.concat(list(dfs.values()), ignore_index=True)\n",
    "scores = dfmerged.groupby([column for column in dfmerged.columns if column != 'seed' and column != 'score'], axis=0).agg({'score': [np.mean, np.std]}, as_index=False)\n",
    "scores = scores.reset_index()\n",
    "df = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dflargest = df[(df['layers'] == 'G.0') &\n",
    "               (df['subset'] == 'learned') &\n",
    "               ((df['metrics'] == 'mrr')) &\n",
    "               (df['eval'] == 'collapse')]\n",
    "dflargest['score_text'] = (100 * dflargest['score']['mean']).apply(lambda x: f\"{x:.2f}\") + '\\stderr{' + (100 * dflargest['score']['std']).apply(lambda x: f\"{x:.2f}\") + '}'\n",
    "dflargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dflargest = df[(df['layers'] == 'G.0') &\n",
    "               (df['subset'] == 'learned') &\n",
    "               ((df['metrics'] == 'recall')) &\n",
    "                ((df['k'] == '10')) &\n",
    "               (df['eval'] == 'collapse')]\n",
    "dflargest['score_text'] = (100 * dflargest['score']['mean']).apply(lambda x: f\"{x:.2f}\") + '\\stderr{' + (100 * dflargest['score']['std']).apply(lambda x: f\"{x:.2f}\") + '}'\n",
    "dflargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "for eos in (\"no_eos\", ):\n",
    "    for subset in (\"learned\", ):\n",
    "        for accum in (\"no_accum\", ):\n",
    "            try:\n",
    "                suffix = f\"{eos}+{subset}+{accum}\"\n",
    "                df = visualize_one_experiment(\n",
    "                    paths=[\n",
    "                        f\"reranker/sweep_v2_ft_pt/seed_{i}/{subset}/{eos}_{accum}/results_detailed.pickle\"\n",
    "                        for i in range(3)\n",
    "                    ],\n",
    "                    suffix=suffix,\n",
    "                    save=False,\n",
    "                    visualize=False,\n",
    "                )\n",
    "                df[\"eos\"] = eos\n",
    "                df[\"subset\"] = subset\n",
    "                df[\"accum\"] = accum\n",
    "                dfs[suffix] = df\n",
    "            except FileNotFoundError:\n",
    "                print(\n",
    "                    \"Couldn't find: \"\n",
    "                    f\"reranker/sweep_v2/seed_0/{subset}/{eos}_{accum}/results_detailed.pickle\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmerged = pd.concat(list(dfs.values()), ignore_index=True)\n",
    "scores = dfmerged.groupby([column for column in dfmerged.columns if column != 'seed' and column != 'score'], axis=0).agg({'score': [np.mean, np.std]}, as_index=False)\n",
    "scores = scores.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest, dflargest = get_max(scores, 'G')\n",
    "display(largest)\n",
    "display(dflargest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest, dflargest = get_max(scores, 'A', no_prefix='G')\n",
    "display(largest)\n",
    "display(dflargest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "for eos in (\"no_eos\", ):\n",
    "    for subset in (\"learned\", ):\n",
    "        for accum in (\"accum\", ):\n",
    "            try:\n",
    "                suffix = f\"{eos}+{subset}+{accum}\"\n",
    "                alpha=0.1\n",
    "                df = visualize_one_experiment(\n",
    "                    paths=[\n",
    "                        f\"reranker/sweep_v2/ensemble_bm25/weight_{alpha}/seed_{i}/{subset}/{eos}_{accum}/results_ensemble_arithmetic_reweight_{alpha}_.pickle\"\n",
    "                        for i in range(3)\n",
    "                    ],\n",
    "                    suffix=suffix,\n",
    "                    save=False,\n",
    "                    visualize=False,\n",
    "                )\n",
    "                df[\"eos\"] = eos\n",
    "                df[\"subset\"] = subset\n",
    "                df[\"accum\"] = accum\n",
    "                dfs[suffix] = df\n",
    "            except FileNotFoundError:\n",
    "                print(\n",
    "                    \"Couldn't find: \"\n",
    "                    f\"reranker/sweep_v2/seed_0/{subset}/{eos}_{accum}/results_detailed.pickle\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmerged = pd.concat(list(dfs.values()), ignore_index=True)\n",
    "scores = dfmerged.groupby([column for column in dfmerged.columns if column != 'seed' and column != 'score'], axis=0).agg({'score': [np.mean, np.std]}, as_index=False)\n",
    "scores = scores.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest, dflargest = get_max(scores, 'G')\n",
    "display(largest)\n",
    "display(dflargest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "for eos in (\"no_eos\", ):\n",
    "    for subset in (\"learned\", ):\n",
    "        for accum in (\"accum\", ):\n",
    "            try:\n",
    "                suffix = f\"{eos}+{subset}+{accum}\"\n",
    "                alpha=0.1\n",
    "                df = visualize_one_experiment(\n",
    "                    paths=[\n",
    "                        f\"reranker/sweep_5100/seed_{i}/{subset}/{eos}_{accum}/results_detailed.pickle\"\n",
    "                        for i in range(3)\n",
    "                    ],\n",
    "                    suffix=suffix,\n",
    "                    save=False,\n",
    "                    visualize=False,\n",
    "                )\n",
    "                df[\"eos\"] = eos\n",
    "                df[\"subset\"] = subset\n",
    "                df[\"accum\"] = accum\n",
    "                dfs[suffix] = df\n",
    "            except FileNotFoundError:\n",
    "                print(\n",
    "                    \"Couldn't find: \"\n",
    "                    f\"reranker/sweep_v2/seed_0/{subset}/{eos}_{accum}/results_detailed.pickle\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmerged = pd.concat(list(dfs.values()), ignore_index=True)\n",
    "scores = dfmerged.groupby([column for column in dfmerged.columns if column != 'seed' and column != 'score'], axis=0).agg({'score': [np.mean, np.std]}, as_index=False)\n",
    "scores = scores.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest, dflargest = get_max(scores, 'G')\n",
    "display(largest)\n",
    "display(dflargest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "83ce25b1c5d65648c98919b9641334dfc26d5cfa225e002eb6106bc7a8478051"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 ('trex': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
